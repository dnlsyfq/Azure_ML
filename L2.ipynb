{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract patterns from data, allowing computers to identify related data, and forecast future outcomes, behaviors, and trends.\n",
    "* application and pattern recognition\n",
    "* anomaly detection\n",
    "* forecasting\n",
    "* recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect Data\n",
    "2. Prepare Data\n",
    "3. Train Model - feature vectorization, scaling , tuning , performance ,\n",
    "4. Evalute Model - from Validation Set , test and compare \n",
    "5. Deploy \n",
    "6. Retrain Model - fresh data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numerical : Int , Float \n",
    "* Time-Series \n",
    "* Speech \n",
    "* Categorical : cardinality\n",
    "* Text \n",
    "* Image\n",
    "\n",
    "\n",
    "Works with no. or vectors . \n",
    "Vectors , array of no. or nested arrays of arrays of numbers\n",
    "\n",
    "* Continous Numerical features\n",
    "    * Scale Data \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Transforming it so that the values fit within some range or scale, such as 0–100 or 0–1. This scaling process will not affect the algorithm output since every value is scaled in the same way. But it can speed up the training process, because now the algorithm only needs to handle numbers less than or equal to 1.\n",
    "\n",
    "\n",
    "Two common approaches to scaling data include standardization and normalization:\n",
    " \n",
    "* **Standardization** : rescale data to have mean = 0 and variance = 1\n",
    " \n",
    "> $\\large \\frac {x-\\mu}{\\sigma}$\n",
    "\n",
    "* **Normalization** : rescale data into range [0,1]\n",
    "\n",
    "> $\\large \\frac {x-x_{min}}{x_{max}-x_{min}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array([-5,10,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2,  0.3,  0.8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(arr - 7)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.75, 1.  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(arr - arr.min()) / (arr.max() - arr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "\n",
    "Categorical data, we need to encode it in some way so that it is represented numerically.\n",
    "There are two common approaches for encoding categorical data: ordinal encoding and one hot encoding.\n",
    "\n",
    "|SKU|Make|\n",
    "|---|---|\n",
    "|908721|Guess|\n",
    "|456552|Tillys|\n",
    "|789921|A&F|\n",
    "|872266|Guess|\n",
    "\n",
    "\n",
    "\n",
    "* Ordinal Encoding : Cons -  implicitly assumes an order across the categories\n",
    "convert the categorical data into integer codes ranging from 0 to (number of categories – 1)\n",
    "\n",
    "|Maker|Encoding|\n",
    "|---|---|\n",
    "|A&F|0|\n",
    "|Guess|1|\n",
    "|TILLYS|2|\n",
    "\n",
    "\n",
    "|SKU|Make|\n",
    "|---|---|\n",
    "|908721|1|\n",
    "|456552|2|\n",
    "|789921|0|\n",
    "|872266|1|\n",
    "\n",
    "* One Hot Encoding : Cons - More Columns \n",
    "\n",
    "|SKU|A&F|Guess|TILLYS|\n",
    "|---|---|---|---|\n",
    "|908721|0|1|0|\n",
    "|456552|0|0|1|\n",
    "|789921|1|0|0|\n",
    "|872266|1|0|0|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data\n",
    "\n",
    "image can be encoded numerically\n",
    "\n",
    "* In grayscale images, each pixel can be represented by a single number, which typically ranges from 0 to 255\n",
    "\n",
    "* In colored images, each pixel can be represented by a vector of three numbers (each ranging from 0 to 255) for the three primary color channels: red, green, and blue\n",
    "\n",
    "\n",
    "RGB Image\n",
    "* Height = 100 px\n",
    "* Width = 100px\n",
    "* Channels = 3\n",
    "\n",
    "The number of channels required to represent the color is known as the color depth or simply depth.\n",
    "* RGB image depth = 3\n",
    "* Grayscale image depth = 1\n",
    "\n",
    "\n",
    "\n",
    "100 * 100 * 3 dimensional vector \n",
    "\n",
    "### Encoding an Image\n",
    "\n",
    "to encode an image. We need to know the following three things about an image to reproduce it:\n",
    "\n",
    "* Horizontal position of each pixel\n",
    "* Vertical position of each pixel\n",
    "* Color of each pixel\n",
    "\n",
    "> can fully encode an image numerically by using a vector with three dimensions. The size of the vector required for any given image would be the height * width * depth of that image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15*15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "> Text normalization is the process of transforming a piece of text into a canonical (official) form\n",
    "\n",
    "\n",
    "the verb to be may show up as is, am, are, and so on. Or a document may contain alternative spellings of a word, such as behavior vs. behaviour. So one step that you will sometimes conduct in processing text is normalization\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "an example of normalization. A lemma is the dictionary form of a word and lemmatization is the process of reducing multiple inflections to that single dictionary form\n",
    "\n",
    "|Original word | Lemmatized word |\n",
    "|---|---|\n",
    "|is|be|\n",
    "|are|be|\n",
    "|am|be|\n",
    "\n",
    "### Stop words \n",
    "are high-frequency words that are unnecessary (or unwanted) during the analysis. For example, when you enter a query like <br>\n",
    "<strong><em>which cookbook has the best pancake recipe</em></strong> <br>\n",
    "into a search engine, the words which and the are far less relevant than cookbook, pancake, and recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized the text (i.e., split each string of text into a list of smaller parts or tokens), removed stop words  and standardized spelling .\n",
    "\n",
    "|Original text|Normalized text|\n",
    "|---|---|\n",
    "|The quick fox|[quick, fox]|\n",
    "|The lazzy dog|[lazy,dog]|\n",
    "|The rabid hare|[rabid,hare]|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "encoding it in a numerical form. The goal here is to identify the particular features of the text that will be relevant to us for the particular task we want to perform—and then get those features extracted in a numerical form that is accessible to the machine learning algorithm.\n",
    "\n",
    "vectorize a word or a sentence:\n",
    "* Term Frequency-Inverse Document Frequency (TF-IDF) vectorization\n",
    "* Word embedding, as done with Word2vec or Global Vectors (GloVe)\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "to give less importance to words that contain less information and are common in documents, such as \"the\" and \"this\"—and to give higher importance to words that contain relevant information and appear less frequently. Thus TF-IDF assigns weights to words that signify their relevance in the documents.\n",
    "\n",
    "||quick|fox|lazy|dog|rabid|hare|\n",
    "|---|---|---|---|---|---|---|\n",
    "|[quick,fox]|0.32|0.23|0|0|0|0|\n",
    "|[lazy,dog]|0|0|0.12|0.23|0|0|\n",
    "|[rabid,hare]|0|0|0|0|0.56|0.12|\n",
    "\n",
    "### Feature Extraction \n",
    "\n",
    "text in the example can be represented by vectors with length 6 since there are 6 words total.\n",
    "\n",
    "```\n",
    "[quick,fox] as (0.32,0.23,0,0,0,0)\n",
    "[lazy,dog] as (0,0,0.12,0.23,0,0)\n",
    "[rabid,hare] as (0,0,0,0,0.56,0.12)\n",
    "```\n",
    "\n",
    "Any vector with the same length can be visualized in the same space. How close one vector is to another can be calculated as vector distance. If two vectors are close to each other, we can say the text represented by the two vectors have a similar meaning or have some connections. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " typical pipeline for text data begins by pre-processing or normalizing the text. This step typically includes tasks such as breaking the text into sentence and word tokens, standardizing the spelling of words, and removing overly common words (called stop words).\n",
    " \n",
    " The next step is feature extraction and vectorization, which creates a numeric representation of the documents.\n",
    " \n",
    " Last, we will feed the vectorized document and labels into a model and start the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Science and Statistical Perspective\n",
    "\n",
    "* Computer Science Perspective\n",
    "```\n",
    "Output Variable = f(InputVariables)\n",
    "```\n",
    "\n",
    "* Statistical Perspective\n",
    "```\n",
    "Dependent Variable = f(Independent Variables)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML\n",
    "\n",
    "* **Libraries**\n",
    "    * Scikit-Learn\n",
    "    * Keras\n",
    "    * Tensorflow\n",
    "    * PyTorch\n",
    "    \n",
    "* **Dev Env**\n",
    "    * Jupyter Notebooks\n",
    "    * Azure Notebooks\n",
    "    * Azure Databricks\n",
    "    * VS Code\n",
    "    * VS\n",
    "    \n",
    "* **Cloud Services**\n",
    "    * MS Azure ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with very large amounts of data, or you need a faster processor, it's a better idea to train and test the model remotely using cloud services such as Microsoft Azure. You can use Azure Data Science Virtual Machine, Azure Databricks, Azure Machine Learning Compute, or SQL server ML services to train and test models and use Azure Kubernetes to deploy models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Services for Machine Learning\n",
    "\n",
    "Core asset management\n",
    "\n",
    "|Feature\t|Description|\n",
    "|---|---|\n",
    "|Datasets|\tDefine, version, and monitor datasets used in machine learning runs.|\n",
    "|Experiments / Runs|\tOrganize machine learning workloads and keep track of each task executed through the service.|\n",
    "|Pipelines\t|Structured flows of tasks to model complex machine learning flows.|\n",
    "|Models\t|Model registry with support for versioning and deployment to production.|\n",
    "|Endpoints|\tExpose real-time endpoints for scoring as well as pipelines for advanced automation.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources Management\n",
    "\n",
    "|Feature|Description|\n",
    "|---|---|\n",
    "|Compute|\tManage compute resources used by machine learning tasks.|\n",
    "|Environments|\tTemplates for standardized environments used to create compute resources.|\n",
    "|Datastores\t|Data sources connected to the service environment (e.g. blob stores, file shares, Data Lake stores, databases).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models vs Algorithms\n",
    "\n",
    "> Models are the specific representations learned from data\n",
    "\n",
    "\n",
    "> Algorithms are the processes of learning\n",
    "\n",
    "\n",
    "objective of ML : to learn model \n",
    "    \n",
    "    \n",
    "    \n",
    "algorithm as a function—we give the algorithm data and it produces a model\n",
    "\n",
    "\n",
    "> **Model = Algorithm(Data)**\n",
    "\n",
    "* Algorithm\n",
    "\n",
    "<center>$y = Wx + b$</center>\n",
    "\n",
    "* Data\n",
    "\n",
    "|x|y|\n",
    "|---|---|\n",
    "|1|1|\n",
    "|2|2|\n",
    "|3|3|\n",
    "\n",
    "* Model\n",
    "\n",
    "<center>$y = 1*x + 0$</center>\n",
    "\n",
    "algorithm as a mathematical tool that can usually be represented by an equation as well as implemented in code\n",
    "\n",
    "\n",
    "Machine learning models are outputs or specific representations of algorithms that run on data. A model represents what is learned by a machine learning algorithm on the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "* Simplifies target function Y to a line\n",
    "* Core idea to obtain fit line that best fits the data\n",
    "* Best fit line is the one which total prediction error for all the data points is small as possible\n",
    "* Error is the distance between the point to the regression line\n",
    "* $B_0$ and $B_n$ coefficients are learned during fitting training\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "$Y = B_0 + B_1*X$\n",
    "\n",
    "* $B_0$ is intercept or bias , to determined the line intercepts the y-axis\n",
    "* $B_1$ is slope, define the slope of the line \n",
    "\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "$Y = B_0 + B_1*X_1 + B_2*X_2 + ... + B_n + * X_n$\n",
    "\n",
    "\n",
    "### 1. Linear Assumption\n",
    "\n",
    "Assumes that the relationship between your input and output\n",
    "\n",
    "### 2. Remove Noise\n",
    " outliers , data cleaning\n",
    " \n",
    "### 3. Remove Collinearity\n",
    "linear regression will overfit when have highly correlated input variables.\n",
    "<br>\n",
    "consider calculating pairwise correlations \n",
    ", remove the most correlated\n",
    "\n",
    "When two variables are collinear, this means they can be modeled by the same line or are at least highly correlated; in other words, one input variable can be accurately predicted by the other\n",
    "\n",
    " Having highly correlated input variables will make the model less consistent, so it's important to perform a correlation check among input variables and remove highly correlated input variables.\n",
    "\n",
    "### 4. Gaussian Distribution\n",
    "\n",
    "reliable prediction when input/output have gaussian distribution\n",
    "\n",
    "Linear regression assumes that the distance between output variables and real data (called residual) is normally distributed. If this is not the case in the raw data, you will need to first transform the data so that the residual has a normal distribution.\n",
    "\n",
    "\n",
    "### 5. Rescale inputs\n",
    "\n",
    "rescale input variables using standardization or normalization \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Algorithm\n",
    "\n",
    "Model representation : $Y = B_0 + B_1 * X $\n",
    "\n",
    "Estimate Slope $B_1$ : $B_1 = \\frac{\\sum_{i=1}^n(x_i - mean(x)) * (y_i -mean(y))}{\\sum_{i=1}^n(x_i - mean(x))^2}$\n",
    "\n",
    "\n",
    "Estimate Intercept $B_0$ : $B_0 = mean(y) - B_1 * mean(x)$\n",
    "\n",
    "\n",
    "Making predictions \n",
    "\n",
    "Estimating Error: Root mean squared error (RMSE): RMSE = $\\sqrt\\frac{\\sum_{i=1}^n(p_i - y_i)^2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The process of finding the best model is essentially a process of finding the coefficients and bias that minimize this error. \n",
    "\n",
    "To calculate this error, we use a cost function. There are many cost functions you can choose from to train a model and the resulting error will be different depending one which cost function you choose. The most commonly used cost function for linear regression:**root mean squared error (RMSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slope = coefficient\n",
    "\n",
    "intercept = bias\n",
    "\n",
    "rmse = cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithm aim to learn a target function $f$ that describes the mapping between data $X$ and output $Y$\n",
    "\n",
    "\n",
    "$Y=f(X)$ + $e$\n",
    "\n",
    "Irreducible error is caused by the data collection process—such as when we don't have enough data or don't have enough data features. In contrast, the model error measures how much the prediction made by the model is different from the true output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric vs Non-parametric\n",
    "\n",
    "machine learning algorithm can be divided into 2 categories :\n",
    "\n",
    "* Parametric\n",
    "    * simplify the mapping to a known functional form \n",
    "        e.g. linear regression  / multiple linear regression / logistic    regression\n",
    "    * Simpler / Faster / Less Training\n",
    "    * highly constrained / limited complexity / poor fit\n",
    "* Non-parametric\n",
    "    * not making assumptions regarding the form of the mapping betwee i/p         and o/p\n",
    "        e.g. k-nearest neighbors \n",
    "    * High Flexibility / Power / High Performance\n",
    "    * more training data / slower / overfitting traing data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning category of ML based on neural networks\n",
    "\n",
    "All DL algorithms are machine learning algorithms.\n",
    "But not all ML algorithm are DL algorithms\n",
    "\n",
    "Deep Learning Advantages\n",
    "\n",
    "1. Learn Arbitrarily complex functions from data \n",
    "2. better accuracy\n",
    "3. better support for big data\n",
    "4. complex features can be learned \n",
    "\n",
    "Disadvantages\n",
    "\n",
    "1. Difficult to explain trained data\n",
    "2. Require significant computational power \n",
    "\n",
    "Classical ML advantages:\n",
    "\n",
    "1. More suitable for small data\n",
    "2. Easier to interpret outcomes\n",
    "3. Cheaper to perform\n",
    "4. Can run on low-end machines\n",
    "5. Does not require large computational power\n",
    "\n",
    "Classical ML disadvantages:\n",
    "\n",
    "1. Difficult to learn large datasets\n",
    "2. Require feature engineering\n",
    "3. Difficult to learn complex functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Supervised Learning|Unsupervised Learning|Reinforcement Learning|\n",
    "|---|---|---|\n",
    "|Learns from data that contains both the inputs and expected outpus|Learns from data that contains only inputs and finds hidden structures in data| Learns how an agent should take actions in an environment to maximise a reward function|\n",
    "| Classification: Outputs are categorical\n",
    " * Regression : Outpus are numerical\n",
    " * Similarity Learning : Learns from examples using similary function (ranking / recommender systems)\n",
    " * Feature Learning : Features are learned using labeled data\n",
    " * Anomaly Detection : Learns from labelled data as normal/abnormal data}\n",
    "|| * Clustering: Assigns entities to clusters or groups\n",
    "||  * Feature Learning: Features are learned from unlabelled data\n",
    "||  * Anomaly detection: Learns from unlabeled data assuming most entities are normal\n",
    "||| * Markov Decision Process: Does not assume knowledge of an exact mathematical model |||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias vs Variance\n",
    "\n",
    "Bias measures how inaccurate the model prediction is in comparison with the true output. It is due to erroneous assumptions made in the machine learning process to simplify the model and make the target function easier to learn. High model complexity tends to have a low bias.\n",
    "\n",
    "Variance measures how much the target function will change if different training data is used. Variance can be caused by modeling the random noise in the training data. High model complexity tends to have a high variance.\n",
    "\n",
    "Parametric and linear algorithms have high bias and low variance \n",
    "Non parametric and nonlinear algorithms have low bias and high variance\n",
    "\n",
    "The prediction error can be viewed as the sum of model error (error coming from the model) and the irreducible error (coming from data collection).\n",
    "\n",
    "\n",
    "```\n",
    "prediction error = Bias error + variance + error + irreducible error\n",
    "```\n",
    "\n",
    "Low bias : less assumptions about target function\n",
    "\n",
    "High bias : more assumptions about target function\n",
    "\n",
    "low variance : changes in training data means small changes of the function estimate \n",
    "\n",
    "high variance : changes in training data means large changes of the function estimate \n",
    "\n",
    "goal of ML : low bias , low variance\n",
    "\n",
    "\n",
    "Low bias means fewer assumptions about the target function. Some examples of algorithms with low bias are KNN and decision trees. Having fewer assumptions can help generalize relevant relations between features and target outputs. In contrast, high bias means more assumptions about the target function. Linear regression would be a good example (e.g., it assumes a linear relationship). Having more assumptions can potentially miss important relations between features and outputs and cause underfitting.\n",
    "\n",
    "Low variance indicates changes in training data would result in similar target functions. For example, linear regression usually has a low variance. High variance indicates changes in training data would result in very different target functions. For example, support vector machines usually have a high variance. High variance suggests that the algorithm learns the random noise instead of the output and causes overfitting.\n",
    "\n",
    "Generally, increasing model complexity would decrease bias error since the model has more capacity to learn from the training data. But the variance error would increase if the model complexity increases, as the model may begin to learn from noise in the training data.\n",
    "\n",
    "The goal of training machine learning models is to achieve low bias and low variance. The optimal model complexity is where bias error crosses with variance error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting vs Underfitting\n",
    "\n",
    "Overfitting refers to the situation in which models fit the training data very well, but fail to generalize to new data.\n",
    "\n",
    "Underfitting refers to the situation in which models neither fit the training data nor generalize to new data.\n",
    "\n",
    "* k-fold cross-validation: it split the initial training data into k subsets and train the model k times. In each training, it uses one subset as the testing data and the rest as training data.\n",
    "* hold back a validation dataset from the initial training data to estimatete how well the model generalizes on new data.\n",
    "* simplify the model. For example, using fewer layers or less neurons to make the neural network smaller.\n",
    "* use more data.\n",
    "* reduce dimensionality in training data such as PCA: it projects training data into a smaller dimension to decrease the model complexity.\n",
    "* Stop the training early when the performance on the testing dataset has not improved after a number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit755892d5a38644fe88af47f4cac36ffe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
